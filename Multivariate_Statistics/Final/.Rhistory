length(y)
x = seq(min(age), max(age), length.out = legnth(y))
x = seq(min(age), max(age), length.out = length(y))
y=predict(g,age=x)
plot(age,wage,col="gray")
plot(x,y)
y=predict(g,age=x,type="response")
library(ISLR)
attach(Wage)
g = glm(wage~age)
x = seq(min(age), max(age), length.out = length(y))
y=predict(g,age=x,type="response")
library(ISLR)
attach(Wage)
g = glm(wage~age)
x = seq(min(age), max(age), 0.01)
y=predict(g,age=x,type="response")
plot(x,y)
length(x)
length(y)
library(ISLR)
attach(Wage)
g = glm(wage~age)
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x),type="response")
plot(x,y)
lines(x,y)
g = glm(wage~age)
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x),type="response")
lines(x,y)
summary(g)
x
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x),type="response")
lines(x,y)
library(ISLR)
attach(Wage)
g = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x),type="response")
lines(x,y)
plot(age,wage,col="gray")
lines(x,y)
library(ISLR)
attach(Wage)
g = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x))
plot(age,wage,col="gray")
lines(x,y)
y
lines(x~y,data=x)
lines(x~y)
lines(y~x)
lines(y,x)
library(ISLR)
attach(Wage)
g = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
y=predict(g,list(age=x))
plot(age,wage,col="gray")
lines(y,x)
library(ISLR)
attach(Wage)
model = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
#y=predict(g,list(age=x))
y=predict.glm(model,newdata=x,type="response")
plot(age,wage,col="gray")
lines(y,x)
#y=predict(g,list(age=x))
y=predict.glm(model,age=x,type="response")
plot(age,wage,col="gray")
lines(y,x)
length(y)
#y=predict(g,list(age=x))
y=predict.glm(model,newdata =x,type="response")
#y=predict(g,list(age=x))
y=predict.glm(model,newdata =list(x),type="response")
plot(age,wage,col="gray")
lines(y,x)
length(y)
length(x)
library(ISLR)
attach(Wage)
model = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
#y=predict(g,list(age=x))
y=predict.glm(model,newdata =list(x),type="response")
plot(age,wage,col="gray")
lines(y,x)
###Kernel Regression in 2-dimensions
###First let's read in our data, how about ye ole faithful coalash.csv
###In this spatial dataset geologists in a crop field in North Carolina
###were interested in modeling the amount of coal ash found in the soil.
###Obviously coal is probably not great for the fertilization process.
###In this data you will find an x and y coordinate for each observation
###as well as a numerical response (coalash).
###Just fyi...datasets like this one are called geostatistical datasets.
setwd("C:\\Users\\User\\Desktop\\School\\Math_537\\practice")
data = read.csv("coalash.csv",h=T)
n = length(data$x)
plot(data$x,data$y,type="n")
text(data$x,data$y,data$coalash,pch=.6)
#We're going to need to keep track of the distances between all of our points
mahalanobis(as.matrix(data[,1:2]),as.matrix(data[1,1:2]),cov(data[,1:2]))
Dij = matrix(0,n,n)
dij = matrix(0,n,n)
for(i in 1:n){
for(j in 1:n){
Dij[i,j] = sqrt((data$x[i] - data$x[j])^2 + (data$y[i] - data$y[j])^2)
dij[i,j]=sqrt(mahalanobis(as.matrix(data[i,1:2]),as.matrix(data[j,1:2]),cov(data[,1:2])))
}
}
#We need to choose our kernel and our bandwidth.  Rule of thumb estimators are out.  Will need to CV.
###Lets go with cross validation r^2, we're goign to choose whichever lambda generates the highest r^2
###for a 10-fold cross validation procedure.
###Select our Kernel...isotropic Gaussian is selected for simplicity at this point...Might want to play with a non-isotropic covariance matrix, up to you really.
###For your challenge problem of the week, and I'll give you an hour + to work on this
###today, you're going to code a full covariance matrix (anisotropic) kernel.
shuffle.index = sample(1:n,n,replace=F)
test.index = vector("list",10)
index.n = floor(n/10)
for(k in 1:10){
test.index[[k]] = shuffle.index[(1+index.n*(k-1)):(index.n*k)]
}
CVR2 = function(h){
prediction = c()
response = c()
for(k in 1:10){
model.data = data[-test.index[[k]],]
test.data = data[test.index[[k]],]
model.n = length(model.data$x)
test.n = length(test.data$x)
Distance.mat = matrix(0,test.n,model.n)
for(i in 1:test.n){
for(j in 1:model.n){
Distance.mat[i,j] = mahalanobis(as.matrix(model.data[i,1:2]),as.matrix(test.data[j,1:2]),h)
#sqrt((model.data$x[j] - test.data$x[i])^2 + (model.data$y[j] - test.data$y[i])^2)
}
}
pred = rep(0,test.n)
for(i in 1:test.n){
pred[i] = sum(1/(sqrt(2*pi)*h^2)*exp(-Distance.mat[i,]^2/(2*h^2))*model.data$coalash)/sum(1/(sqrt(2*pi)*h^2)*exp(-Distance.mat[i,]^2/(2*h^2)))
}
prediction = c(prediction,pred)
response = c(response,test.data$coalash)
}
SSR = sum((response - prediction)^2)
SSR
}
h=optim(matrix(c(1,2,2,1),2,2),CVR2)$par
h
Kern.Reg = rep(0,n)
for(i in 1:n)
{
Kern.Reg[i] = sum(1/(sqrt(2*pi)*h^2)*exp(-Dij[i,-i]^2/(2*h^2))*data$coalash[-i])/sum(1/(sqrt(2*pi)*h^2)*exp(-Dij[i,-i]^2/(2*h^2)))
#Kern.Reg[i,] = sum(1/(sqrt(2*pi)*h^2)*exp(-Dij[i,-i]^2*(2*solve(h)^2))*data$coalash[-i])/sum(1/(sqrt(2*pi)*h^2)*exp(-Dij[i,-i]^2*(2*solve(h)^2)))
}
plot(data$x,data$y,type="n")
text(data$x,data$y,Kern.Reg,pch=.3)
library(ellipse)
library(Rfast)
setwd("C:\\Users\\User\\Desktop\\School\\Math_537\\Test1")
data=read.csv("genderwage.csv")
############################################
#1 a)
x=data$female
y=data$male
n=length(x)
variance=matrix(var(data),ncol=2)/n
muX=mean(x)
muY=mean(y)
mu=matrix(c(muX,muY),2,1)
muT=mean(cbind(x,y))
findMu=function(x,mu,variance)
{
xy=c(x,x)
result=t(mu-xy)%*%solve(variance)%*%(mu-xy)
return(result)
}
result=optim(par=20,fn=findMu,mu=mu,variance=variance)
mu0=matrix(c(result$par,result$par),2,1)
t2=t(mu-mu0)%*%solve(variance)%*%(mu-mu0)
pval=2*(1-pf((n-2)/((n-1)*2)*t2,2,n-2))
pval
############################################
#1 b)
eig=eigen(variance)
lam1=eig$values[1]
lam2=eig$values[2]
v1=eig$vectors[,1]
v2=eig$vectors[,2]
dist =(2*(n-1)/(n-2))*qf(.95,2,n-2)
a1=v1*sqrt(lam1)*sqrt(dist)
a2=v2*sqrt(lam2)*sqrt(dist)
plot(x,y,xlim=(c(15,26)),ylim=(c(17,29)))
points(muX,muY,pch=19,cex=1,col=3)
lines(ellipse(variance,centre=c(muX,muY)),col=3)
lines(c(muX,muX+a1[1]),c(muY,muY+a1[2]),lwd=.5,col=2)
lines(c(muX,muX+a2[1]),c(muY,muY+a2[2]),lwd=.5,col=2)
lines(c(0,28),c(0,28),lwd=.5,col=2)
points(mu0[1],mu0[2],pch=19,cex=1,col=2)
lowerX=muX-qt(.975,n-1)*sd(x)/sqrt(n)
upperX=muX+qt(.975,n-1)*sd(x)/sqrt(n)
print(lowerX)
print(muX)
print(upperX)
lowerY=muY-qt(.975,n-1)*sd(y)/sqrt(n)
upperY=muY+qt(.975,n-1)*sd(y)/sqrt(n)
print(lowerY)
print(muY)
print(upperY)
library(ellipse)
library(Rfast)
setwd("C:\\Users\\User\\Desktop\\School\\Math_537\\Test1")
data=read.csv("genderwage.csv")
############################################
#1 a)
x=data$female
y=data$male
n=length(x)
variance=matrix(var(data),ncol=2)/n
muX=mean(x)
muY=mean(y)
mu=matrix(c(muX,muY),2,1)
muT=mean(cbind(x,y))
findMu=function(x,mu,variance)
{
xy=c(x,x)
result=t(mu-xy)%*%solve(variance)%*%(mu-xy)
return(result)
}
result=optim(par=20,fn=findMu,mu=mu,variance=variance)
mu0=matrix(c(result$par,result$par),2,1)
t2=t(mu-mu0)%*%solve(variance)%*%(mu-mu0)
pval=2*(1-pf((n-2)/((n-1)*2)*t2,2,n-2))
pval
############################################
#1 b)
eig=eigen(variance)
lam1=eig$values[1]
lam2=eig$values[2]
v1=eig$vectors[,1]
v2=eig$vectors[,2]
dist =(2*(n-1)/(n-2))*qf(.95,2,n-2)
a1=v1*sqrt(lam1)*sqrt(dist)
a2=v2*sqrt(lam2)*sqrt(dist)
plot(x,y,xlim=(c(15,26)),ylim=(c(17,29)))
points(muX,muY,pch=19,cex=1,col=3)
lines(ellipse(variance,centre=c(muX,muY)),col=3)
lines(c(muX,muX+a1[1]),c(muY,muY+a1[2]),lwd=.5,col=2)
lines(c(muX,muX+a2[1]),c(muY,muY+a2[2]),lwd=.5,col=2)
lines(c(0,28),c(0,28),lwd=.5,col=2)
points(mu0[1],mu0[2],pch=19,cex=1,col=2)
############################################
#1 c)
lowerX=muX-qt(.975,n-1)*sd(x)/sqrt(n)
upperX=muX+qt(.975,n-1)*sd(x)/sqrt(n)
print(lowerX)
print(muX)
print(upperX)
lowerY=muY-qt(.975,n-1)*sd(y)/sqrt(n)
upperY=muY+qt(.975,n-1)*sd(y)/sqrt(n)
print(lowerY)
print(muY)
print(upperY)
############################################
#1 d)
xa = mean(x) - qt(.9875,length(x)-1)*sd(x)/sqrt(length(x))
xb = mean(x) + qt(.9875,length(x)-1)*sd(x)/sqrt(length(x))
ya = mean(y) - qt(.9875,length(y)-1)*sd(y)/sqrt(length(y))
yb = mean(y) + qt(.9875,length(y)-1)*sd(y)/sqrt(length(y))
plot(x,y,xlim=(c(15,26)),ylim=(c(17,29)))
points(muX,muY,pch=19,cex=1,col=3)
lines(ellipse(variance,centre=c(muX,muY)),col=3)
lines(c(muX,muX+a1[1]),c(muY,muY+a1[2]),lwd=.5,col=2)
lines(c(muX,muX+a2[1]),c(muY,muY+a2[2]),lwd=.5,col=2)
lines(c(0,28),c(0,28),lwd=.5,col=2)
points(mu0[1],mu0[2],pch=19,cex=1,col=2)
lines(c(xa,xb),c(ya,ya),lty=2,col=4)
lines(c(xa,xb),c(yb,yb),lty=2,col=4)
lines(c(xa,xa),c(ya,yb),lty=2,col=4)
lines(c(xb,xb),c(ya,yb),lty=2,col=4)
############################################
#1 e)
muM=matrix(c(22.5,24.5),2,1)
muR=matrix(c(21.5,26),2,1)
sigM=matrix(c(12,8,8,12),2,2)
sigR=matrix(c(9,8,8,16),2,2)
likeRatio=prod(dmvnorm(as.matrix(data),muM,sigM))/prod(dmvnorm(as.matrix(data),muR,sigR))
#Rachel is the winner, sorry M
############################################
#1 f)
eig=eigen(cov(as.matrix(data)))
vec=eig$vectors
l=eig$values
eMu0=vec[,1]%*%mu0
eX=as.matrix(data)%*%vec[,1]
t=(mean(eX)-eMu0)/(sd(eX)/sqrt(n))
2*(1-pt(t,n-1))
library(mvtnorm)
library(RVAideMemoire)
library(rstatix)
library(mnormt)
library(nFactors)
setwd('C:\\Users\\User\\Desktop\\School\\Math_537\\Test2')
data=read.table('Apple.txt', header=T)
head(data)
######################################################
#1 a)
y=cbind(data$y1,data$y2,data$y3,data$y4)
model=manova(y~as.factor(Rootstock),data=data)
lam=summary.manova(model,test="Wilks")$stats[3]
lam
######################################################
#1 b)
temp=summary.manova(model,test="Wilks")
temp$SS$Residuals
model$residuals
mqqnorm(model$residuals)
#fairly normal
box_m(y, data$Rootstock)
# With a p-value of .711 this implies that each y_i came from similar variances.
######################################################
#1 c)
n=10000
lam2=matrix(0,1,n)
data2=as.matrix(data)
ind=sample(1:48,replace=T)
final=cbind((data2[,1]),(data2[ind,2:5]))
results= final[,2:5]
y=cbind(final[,2],final[,3],final[,4],final[,5])
model=manova(y~as.factor(final[,1]),data=as.data.frame(final))
lam2[1]=summary.manova(model,test="Wilks")$stats[3]
for(i in 2:n)
{
ind=sample(1:48,replace=T)
final=cbind((data2[,1]),(data2[ind,2:5]))
results=results+final[,2:5]
y=cbind(final[,2],final[,3],final[,4],final[,5])
model=manova(y~as.factor(final[,1]),data=as.data.frame(final))
lam2[i]=summary.manova(model,test="Wilks")$stats[3]
}
sum(lam2<lam)/n
results=results/n
results=cbind((data2[,1]),results)
y=cbind(results[,2],results[,3],results[,4],results[,5])
model=manova(y~as.factor(results[,1]),data=as.data.frame(results))
mqqnorm(model$residuals)
######################################################
#2 a)
data2=read.table('drivPoints.txt', sep=",", header=T)
data2F=scale(data2[,6:dim(data2)[2]],scale=F)
pca = prcomp(data2F)
pca
plot(pca,type="l")
source("ggbiplot2.R")
componet=pca$rotation[,1:2]
#All principle components printed
g = ggbiplot(pca, obs.scale = 1, var.scale = 1,
groups = as.factor(data2[,4]), ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
g
#Only the first two principle componenets printed (important ones)
g = ggbiplot(pca, obs.scale = 1, var.scale = 1,
groups = as.factor(data2[,4]), ellipse = TRUE,
circle = TRUE,numComp=2)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
g
head(data2)
######################################################
#2 b)
X=data2F
f1=factanal(data2F,factors=9,rotation="varimax")
f2=factanal(data2F,factors=4,rotation="varimax")
ev <- eigen(cor(X)) # get eigenvalues
ap <- parallel(subject=nrow(X),var=ncol(X),
rep=100,cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
library(MASS)
library(glmnet)
library(assist)
library(pls)
setwd("C:\\Users\\User\\Desktop\\School\\Math_537\\Final")
data=read.csv("College.csv")
data$exclu=100*(data$Apps-data$Accept)/data$Apps+100*(data$Enroll/data$Accept)
Private=ifelse(data$Private=="Yes", 1,0)
dataS=scale(cbind(Private,data[,7:dim(data)[2]-1]),center=T,scale=T)
y=scale(data[,dim(data)[2]],center=T,scale=T)
modelRes=matrix(0,30,1000)
for(k in 1:1000)
{
j=5
set.seed(k)
folds=cut(seq(1,nrow(data)),breaks=j,labels=F)
testIndexes=matrix(0,length(folds[folds==1]),j)
for (i in 1:j)
{
if(length(folds[folds==i])==156)
{
testIndexes[,i]=which(folds==i,arr.ind=T)
}
else
{
testIndexes[,i]=c(which(folds==i,arr.ind=T),0)
}
}
pca=prcomp(dataS)
plot(pca,type="l")
#Not a big gain for any addition components after 3, will pick 3.
i=1
for(i in 1:j)
{
testDatax=dataS[testIndexes[,i],]
trainDatax=dataS[-testIndexes[,i],]
testDatay=y[testIndexes[,i]]
trainDatay=y[-testIndexes[,i]]
trainData=data.frame(cbind(trainDatay,trainDatax))
testData=data.frame(cbind(testDatay,testDatax))
ols <- lm(trainData$trainDatay~ ., data=trainData)#trainData[,2:dim(trainData)[2]], data=trainData)
pred=predict.glm(ols,newdata=data.frame(testDatax),type="response")
modelRes[1,k]=modelRes[1,k]+sum((testDatay-pred)^2)
cvLam=cv.glmnet(data.matrix(trainDatax),y=trainDatay,alpha=0,nfolds=j)$lambda.min
ridge=glmnet(trainDatax,trainDatay,alpha=0,lambda=cvLam)
pred=predict(ridge,newx=data.matrix(testDatax),s=cvLam)
modelRes[2,k]=modelRes[2,k]+sum((testDatay-pred)^2)
cvLam=cv.glmnet(data.matrix(trainDatax),y=trainDatay,alpha=1,nfolds=j)$lambda.min
lasso=glmnet(data.matrix(trainDatax),trainDatay,alpha=1,lambda=cvLam)
pred=predict(lasso,newx=data.matrix(testDatax),s=cvLam)
modelRes[3,k]=modelRes[3,k]+sum((testDatay-pred)^2)
cvLam=cv.glmnet(data.matrix(trainDatax),y=trainDatay,alpha=.5,nfolds=j)$lambda.min
elast=glmnet(trainDatax,trainDatay,alpha=.5,lambda=cvLam)
pred=predict(elast,newx=data.matrix(testDatax),s=cvLam)
modelRes[4,k]=modelRes[4,k]+sum((testDatay-pred)^2)
for(j in 1:13)
{
mPCR=pcr(trainDatay~trainDatax ,ncomp=j)
pred=predict(mPCR,testDatax,ncomp=j)
modelRes[5+j-1,k]=modelRes[5+j-1,k]+sum((testDatay-pred)^2)
mPLSR = plsr(trainDatay~trainDatax,ncomp=j)
pred=predict(mPLSR,testDatax,ncomp=j)
modelRes[6+13+j-2,k]=modelRes[6+13+j-2,k]+sum((testDatay-pred)^2)
}
}
if(k%%100==0)
{
print(k)
}
}
name=c('LM','Ridge','Lasso','Elastic')
for(i in 1:13)
{
name=c(name,paste0("PCR: Num Comp=",i))
}
for(i in 1:13)
{
name=c(name,paste0("PLSR: Num Comp=",i))
}
rownames(modelRes)=as.vector(name)
winningModel=apply(modelRes,2,which.min)
winningModel[winningModel==2]="Ridge"
winningModel[winningModel==3]="Lasso"
winningModel[winningModel==4]="Elastic"
output=as.data.frame(table(winningModel))
output=output[order(-output$Freq),]
output
rowMeans(modelRes)
library(ISLR)
attach(Wage)
model = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
#y=predict(g,list(age=x))
y2=cv.glmnet(age,y=wage,alpha=0,nfolds=3)
y=predict.glm(model,newdata =x,type="response")
plot(age,wage,col="gray")
#y=predict(g,list(age=x))
y2=cv.glmnet(age,y=list(wage),alpha=0,nfolds=3)
#y=predict(g,list(age=x))
y2=cv.glmnet(age,y=list(wage),alpha=0,nfolds=2)
#y=predict(g,list(age=x))
y2=cv.glmnet(age,y=list(wage),alpha=0,nfolds=1)
library(ISLR)
attach(Wage)
model = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
#y=predict(g,list(age=x))
cvlam=cv.glmnet(age,y=wage,alpha=0,nfolds=j)$lambda.min
ridge=glmnet(trainDatax,trainDatay,alpha=0,lambda=cvLam)
pred=predict(ridge,newx=data.matrix(x),s=cvLam)
library(ISLR)
attach(Wage)
Wage
model = glm(wage~age)
summary(g)
x = seq(min(age), max(age), 0.01)
#y=predict(g,list(age=x))
cvlam=cv.glmnet(age,y=wage,alpha=0,nfolds=j)$lambda.min
